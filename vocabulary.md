LLM (Large Language Model)-A large-scale neural network trained on massive text data that can generate meaningful texts, answer questions, and solve cognitive problems.

Fine-tuning-Further training of a completed model on a highly specialized dataset to improve its performance in a specific application.

Prompt Engineering-A method for formulating optimal queries to artificial intelligence to obtain the most relevant and high-quality results.

RAG (Retrieval-Augmented Generation)-A technology that allows a model to extract relevant data from external sources or knowledge bases before generating a response.


Token-A basic text unit (a word fragment or symbol) that a neural network uses to analyze and process information.

Inference-The stage of a model's operation during which it generates a response based on a received query in real time.

Dataset-A collection of information used to train and develop the model's capabilities.

Embeddings-A vector representation of text in numerical format that allows for determining the semantic similarity between fragments.

Model Hallucination-A phenomenon in which a model generates unreliable or fictitious information, passing it off as fact.

Generative Adversarial Network (GAN)-A neural network architecture consisting of two components—a generator and a validator—used to generate visual and audio content.
