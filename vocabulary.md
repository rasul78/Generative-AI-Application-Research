LLM (Large Language Model) is a large-scale neural network trained on massive volumes of text data, capable of generating meaningful texts, answering questions, and solving cognitive problems.
For example, when you ask a chatbot for resume writing advice, it's the LLM that analyzes your request and generates a structured response based on millions of examples from its training database.


Fine-tuning is the additional training of an existing model on a highly specialized dataset to improve its performance in a specific application.
For example, a basic model understands general language well, but for processing medical documents, it is further trained on clinical notes and scientific articles—this way, it begins to better recognize terms like "hypertension" or "arrhythmia."


Prompt Engineering is a technique for formulating optimal queries to artificial intelligence to obtain the most relevant and high-quality results. Instead of a vague "tell me about marketing," an experienced user might write: "Create a content marketing plan for a startup in the eco-friendly packaging industry; the target audience is cafe owners aged 25-40." The result will be much more precise.


RAG (Retrieval-Augmented Generation) is a technology that allows a model to extract relevant data from external sources or knowledge bases before generating a response.
Imagine a corporate assistant: when an employee asks about vacation policies, the system first finds the relevant section in the company's internal documentation and then formulates a response based on that specific data, not general knowledge.


Token is a basic unit of text (a word fragment or symbol) that a neural network uses to analyze and process information.
The word "unpredictability" can be broken down into tokens like this: "unpredictable-ness." Models work with precisely these tokens, which helps them process even unfamiliar words.

Inference is the stage of a model's operation during which it generates a response based on the received request in real time. When you press "send" in a chat, inference begins: the model processes your text and produces a result in a few seconds, using its existing "knowledge" without additional training.

A dataset is a collection of information used to train and develop a model's capabilities.


To create a translator, a dataset is compiled from millions of parallel texts in different languages—for example, news articles, film subtitles, and official UN documents.


Embeddings are vector representations of text in numerical format that allow one to determine the semantic similarity between fragments.
Thanks to embeddings, the system understands that the words "car" and "vehicle" are very close in meaning (their vectors are located close to each other), while "car" and "apple" are distant.

Model Hallucination is a phenomenon in which a model generates false or fictitious information, passing it off as fact. Ask a chatbot about a little-known scientist and you might get a biography with plausible but completely fictitious details—dates, awards, publications—that don't actually exist.


A Generative Adversarial Network (GAN) is a neural network architecture consisting of two components—a generator and a validator—used to generate visual and audio content.
One component attempts to create a realistic facial image, while the other checks how similar it is to a real photograph. During this "competition," the quality of the generated image continually improves until the result becomes indistinguishable from reality.
